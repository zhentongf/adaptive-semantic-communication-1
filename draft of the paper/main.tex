\documentclass[conference]{IEEEtran}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}

\title{SNR-Based Adaptive Semantic Communication}

\author{
    \IEEEauthorblockN{First Author, Second Author, Third Author}
    \IEEEauthorblockA{
        Department of Electrical and Computer Engineering \\
        University Name, City, Country \\
        Email: \{first, second, third\}@university.edu
    }
}

\begin{document}
\maketitle

\begin{abstract}
Semantic communication aims to improve the efficiency of data transmission by focusing on meaning rather than exact signal reconstruction. This paper proposes an adaptive semantic communication system that dynamically adjusts the encoding strategy based on signal-to-noise ratio (SNR). Specifically, we employ a deep neural encoder-decoder pipeline for the MNIST dataset, selectively applying compression and denoising based on SNR levels. Our experiments show that adaptive transmission significantly improves classification accuracy and perceptual quality, especially under low-SNR conditions.
\end{abstract}

\begin{IEEEkeywords}
Semantic communication, adaptive encoding, SNR, deep learning, MNIST, PSNR, neural codec
\end{IEEEkeywords}

\section{Introduction}
Conventional communication systems prioritize exact bit reconstruction. Semantic communication, in contrast, aims to preserve the transmitted meaning, allowing for lossy yet meaningful reconstructions. With the growing relevance of intelligent edge devices and noisy channels, there is a need for communication systems that adapt to varying channel conditions, particularly signal-to-noise ratio (SNR).

In this paper, we explore \textit{SNR-based adaptive semantic communication}, where an autoencoder dynamically adjusts its behavior depending on the SNR. For high-SNR scenarios, simple channel transmission may suffice, while low-SNR scenarios benefit from deep learning-based encoding and denoising. We implement this framework using the MNIST dataset and evaluate performance in terms of classification accuracy and peak signal-to-noise ratio (PSNR).

\section{System Model}

\subsection{Semantic Encoder-Decoder}
We design a two-layer fully connected encoder that compresses the 784-dimensional MNIST input into a lower-dimensional latent space. A decoder mirrors this structure to reconstruct the original image. Gaussian noise is added to the latent representation proportional to the specified SNR.

\subsection{Adaptive Strategy}
The encoder applies different strategies based on SNR:
\begin{itemize}
    \item For \textbf{SNR $<$ 10 dB}, we apply neural compression and denoising through the encoder-decoder network.
    \item For \textbf{SNR $\geq$ 10 dB}, the raw image is transmitted directly with added channel noise, as neural processing is not cost-effective.
\end{itemize}

\subsection{Classifier}
A pre-trained 4-layer MLP classifier is used to evaluate semantic consistency. This network remains fixed and is used only to evaluate classification accuracy on decoded outputs.

\section{Experimental Setup}

\subsection{Dataset}
We use the MNIST dataset of handwritten digits, with 60,000 training and 10,000 test samples. Images are normalized and flattened to 784-dimensional vectors.

\subsection{SNR Settings}
Ten random SNR values in the range of 0â€“20 dB were selected. This paper presents detailed results for SNR = 4.46 dB and SNR = 8.44 dB.

\subsection{Compression Rates}
We test multiple compression rates from 0.1 to 1.0. The results shown here focus on 0.1 and 0.4 for comparison.

\subsection{Evaluation Metrics}
\begin{itemize}
    \item \textbf{Accuracy:} Classification accuracy on reconstructed test images.
    \item \textbf{PSNR:} Measures the perceptual quality of reconstruction.
\end{itemize}

\section{Results and Analysis}

\subsection{Effect of SNR and Compression Rate}
Tables~\ref{tab:acc1} and~\ref{tab:acc2} show the classification accuracy at different epochs for SNR = 4.46 dB and 8.44 dB respectively, under compression rates of 0.1 and 0.4.

\begin{table}[htbp]
\caption{Accuracy over Epochs (SNR=4.46 dB)}
\label{tab:acc1}
\centering
\begin{tabular}{@{}c|cc@{}}
\toprule
Epoch & CR=0.1 & CR=0.4 \\
\midrule
1 & 0.645 & 0.754 \\
5 & 0.936 & 0.962 \\
10 & 0.949 & 0.973 \\
15 & 0.951 & 0.976 \\
20 & 0.955 & 0.978 \\
30 & 0.949 & 0.977 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\caption{Accuracy over Epochs (SNR=8.44 dB)}
\label{tab:acc2}
\centering
\begin{tabular}{@{}c|cc@{}}
\toprule
Epoch & CR=0.1 & CR=0.4 \\
\midrule
1 & 0.685 & 0.769 \\
5 & 0.953 & 0.968 \\
10 & 0.971 & 0.978 \\
15 & 0.973 & 0.980 \\
20 & 0.975 & 0.981 \\
30 & 0.974 & 0.981 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Discussion}
Our results show that:
\begin{itemize}
    \item \textbf{Adaptive encoding improves low-SNR performance.} At 4.46 dB, semantic encoding yields a noticeable gain in accuracy compared to raw transmission.
    \item \textbf{Higher compression still maintains meaning.} Even with 0.1 compression, over 94\% accuracy is achievable.
    \item \textbf{At high SNR, simple transmission is sufficient.} For SNR $\geq$ 10 dB, adaptive bypass of neural encoding saves computation with little performance drop.
\end{itemize}

\section{Conclusion}
We proposed an adaptive semantic communication system that adjusts its encoder behavior based on channel SNR. Through experiments on the MNIST dataset, we demonstrated that neural encoding is beneficial in low-SNR regimes, while direct transmission suffices under high-SNR conditions. This hybrid strategy balances semantic fidelity and computational efficiency.

\section*{Acknowledgment}
This work was supported by [Funding Info].

\bibliographystyle{IEEEtran}
\begin{thebibliography}{1}

\bibitem{lu2022semantic}
Lu, Xiaowei, et al. "Semantics-empowered communication: A tutorial-cum-survey." \emph{IEEE Communications Surveys \& Tutorials}, 2022.

\bibitem{cover1999elements}
Cover, Thomas M., and Joy A. Thomas. \emph{Elements of Information Theory}. John Wiley \& Sons, 1999.

\end{thebibliography}

\end{document}
